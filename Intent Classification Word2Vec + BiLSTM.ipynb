{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052f967b-bb6f-4d1e-a38a-edf7a6bb388c",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d9a944-4a5a-4889-8145-8cfd07020496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import Layer, Input, Embedding, Bidirectional, LSTM, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bd0182-fd31-4754-ae5b-f8478246fca8",
   "metadata": {},
   "source": [
    "### Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f66b694-d22f-4416-8bc2-83ee489f5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "df_train = pd.read_csv(\"dialog.csv\")\n",
    "df_test = pd.read_csv(\"dialog_test.csv\")\n",
    "\n",
    "# Define a simple text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation/numbers\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Create a new column with cleaned text\n",
    "df_train[\"clean_text\"] = df_train[\"User Dialog\"].apply(clean_text)\n",
    "df_test[\"clean_text\"] = df_test[\"User Dialog\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c08c41d-833d-43bb-a041-dd41ec7fc450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Dialog</th>\n",
       "      <th>Intent Class</th>\n",
       "      <th>Subcategory</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I submitted my claim transfer request weeks ag...</td>\n",
       "      <td>Contract Modification</td>\n",
       "      <td>Claim Transfer Requests</td>\n",
       "      <td>Negative</td>\n",
       "      <td>submitted claim transfer request weeks ago tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seriously, are you even listening to me? I've ...</td>\n",
       "      <td>Contract Modification</td>\n",
       "      <td>Claim Transfer Requests</td>\n",
       "      <td>Negative</td>\n",
       "      <td>seriously even listening ive asked three times...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'd like to request a transfer of my claim to ...</td>\n",
       "      <td>Contract Modification</td>\n",
       "      <td>Claim Transfer Requests</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>id like request transfer claim different adjus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm inquiring about the possibility of transfe...</td>\n",
       "      <td>Contract Modification</td>\n",
       "      <td>Claim Transfer Requests</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>im inquiring possibility transferring claim st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you so much for helping me transfer my c...</td>\n",
       "      <td>Contract Modification</td>\n",
       "      <td>Claim Transfer Requests</td>\n",
       "      <td>Positive</td>\n",
       "      <td>thank much helping transfer claim really appre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         User Dialog           Intent Class  \\\n",
       "0  I submitted my claim transfer request weeks ag...  Contract Modification   \n",
       "1  Seriously, are you even listening to me? I've ...  Contract Modification   \n",
       "2  I'd like to request a transfer of my claim to ...  Contract Modification   \n",
       "3  I'm inquiring about the possibility of transfe...  Contract Modification   \n",
       "4  Thank you so much for helping me transfer my c...  Contract Modification   \n",
       "\n",
       "               Subcategory Sentiment  \\\n",
       "0  Claim Transfer Requests  Negative   \n",
       "1  Claim Transfer Requests  Negative   \n",
       "2  Claim Transfer Requests   Neutral   \n",
       "3  Claim Transfer Requests   Neutral   \n",
       "4  Claim Transfer Requests  Positive   \n",
       "\n",
       "                                          clean_text  \n",
       "0  submitted claim transfer request weeks ago tak...  \n",
       "1  seriously even listening ive asked three times...  \n",
       "2  id like request transfer claim different adjus...  \n",
       "3  im inquiring possibility transferring claim st...  \n",
       "4  thank much helping transfer claim really appre...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e0315a-8db9-445e-87eb-c40347392220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Dialog</th>\n",
       "      <th>Intent Class</th>\n",
       "      <th>Subcategory</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm looking to transfer my claim to a differen...</td>\n",
       "      <td>Contract Modification</td>\n",
       "      <td>Claim Transfer Requests</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>im looking transfer claim different adjuster w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Could you please update the email address asso...</td>\n",
       "      <td>Contract Modification</td>\n",
       "      <td>Email Update</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>could please update email address associated p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm calling to check on the status of my claim...</td>\n",
       "      <td>Contract Modification</td>\n",
       "      <td>Claim Processing</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>im calling check status claim processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I need to file a claim for damage to my RV. Wh...</td>\n",
       "      <td>Contract Modification</td>\n",
       "      <td>RV Claims Assistance</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>need file claim damage rv assist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What's the current status of my contract trans...</td>\n",
       "      <td>Contract Transfer</td>\n",
       "      <td>Contract Transfer Status</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>whats current status contract transfer request</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         User Dialog           Intent Class  \\\n",
       "0  I'm looking to transfer my claim to a differen...  Contract Modification   \n",
       "1  Could you please update the email address asso...  Contract Modification   \n",
       "2  I'm calling to check on the status of my claim...  Contract Modification   \n",
       "3  I need to file a claim for damage to my RV. Wh...  Contract Modification   \n",
       "4  What's the current status of my contract trans...      Contract Transfer   \n",
       "\n",
       "                Subcategory Sentiment  \\\n",
       "0   Claim Transfer Requests   Neutral   \n",
       "1              Email Update   Neutral   \n",
       "2          Claim Processing   Neutral   \n",
       "3      RV Claims Assistance   Neutral   \n",
       "4  Contract Transfer Status   Neutral   \n",
       "\n",
       "                                          clean_text  \n",
       "0  im looking transfer claim different adjuster w...  \n",
       "1  could please update email address associated p...  \n",
       "2           im calling check status claim processing  \n",
       "3                   need file claim damage rv assist  \n",
       "4     whats current status contract transfer request  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6f368c-8230-4118-becc-db54c151fcb5",
   "metadata": {},
   "source": [
    "### Preparing Word Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb460f9f-6bfb-45cd-af59-a09b21bbd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    google_news = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "except FileNotFoundError:\n",
    "    print(\"Downloading pre-trained Word2Vec model...\")\n",
    "    google_news = api.load(\"word2vec-google-news-300\")\n",
    "    google_news.save_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "    print(\"Model downloaded and saved.\")\n",
    "\n",
    "embedding_dim = google_news.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ebd7701-b7a7-4f90-a6ca-618932209370",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train[\"clean_text\"])\n",
    "\n",
    "# Convert training texts to sequences.\n",
    "train_sequences = tokenizer.texts_to_sequences(df_train[\"clean_text\"])\n",
    "# Determine maximum length based on training data.\n",
    "max_len = max(len(seq) for seq in train_sequences)\n",
    "# Pad training sequences.\n",
    "X_train = pad_sequences(train_sequences, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "# Convert test texts to sequences using the same tokenizer.\n",
    "test_sequences = tokenizer.texts_to_sequences(df_test[\"clean_text\"])\n",
    "# Pad test sequences using the same maximum length.\n",
    "X_test = pad_sequences(test_sequences, maxlen=max_len, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cf3a422-d2f2-4346-abd5-a7e937dcf1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding domain specific vocabulary. Need to expand.\n",
    "domain_words = [\n",
    "    \"insurance\", \"policy\", \"claim\", \"deductible\", \"premium\", \"coverage\",\n",
    "    \"accident\", \"collision\", \"damage\", \"repair\", \"replace\", \"reimbursement\",\n",
    "    \"payment\", \"billing\", \"quote\", \"adjuster\", \"agent\", \"renewal\",\n",
    "    \"cancellation\", \"vehicle\", \"car\", \"auto\", \"driver\", \"license\",\n",
    "    \"status\", \"update\", \"process\", \"assistance\", \"modification\", \"inquiry\",\n",
    "    \"report\", \"totaled\", \"rental\", \"roadside\", \"assistance\", \"gap\", \"liability\",\n",
    "    \"comprehensive\", \"collision\", \"uninsured\", \"underinsured\", \"waiver\", \"discount\",\n",
    "    \"approved\", \"denied\", \"pending\", \"estimate\", \"shop\", \"mechanic\", \"parts\",\n",
    "    \"labor\", \"towing\", \"wreck\", \"fault\", \"accidentforgiveness\", \"goodstudent\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "654e3f65-cff4-44b0-b0d0-6a7fb12b25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the embedding matrix using the tokenizer's vocabulary.\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in google_news:\n",
    "        embedding_matrix[i] = google_news[word]\n",
    "    else:\n",
    "        # If the word is not in GoogleNews, assign a random vector.\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed736c0c-b655-45ff-8fbc-614a1e64e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is assumed that both datasets share the same label space.\n",
    "intent_classes = sorted(df_train[\"Intent Class\"].unique())\n",
    "subcategory_classes = sorted(df_train[\"Subcategory\"].unique())\n",
    "sentiment_classes = sorted(df_train[\"Sentiment\"].unique())\n",
    "\n",
    "intent_to_index = {intent: idx for idx, intent in enumerate(intent_classes)}\n",
    "subcategory_to_index = {sub: idx for idx, sub in enumerate(subcategory_classes)}\n",
    "sentiment_to_index = {sent: idx for idx, sent in enumerate(sentiment_classes)}\n",
    "\n",
    "# Convert labels for training set.\n",
    "y_intent_train = np.array([intent_to_index[intent] for intent in df_train[\"Intent Class\"]])\n",
    "y_sub_train = np.array([subcategory_to_index[sub] for sub in df_train[\"Subcategory\"]])\n",
    "y_sentiment_train = np.array([sentiment_to_index[sent] for sent in df_train[\"Sentiment\"]])\n",
    "\n",
    "# Convert labels for test set.\n",
    "y_intent_test = np.array([intent_to_index[intent] for intent in df_test[\"Intent Class\"]])\n",
    "y_sub_test = np.array([subcategory_to_index[sub] for sub in df_test[\"Subcategory\"]])\n",
    "y_sentiment_test = np.array([sentiment_to_index[sent] for sent in df_test[\"Sentiment\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0472a396-6355-48c2-8219-b5b5e6f2d8e6",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "836c900d-835b-4c53-9476-559ab6fa86fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 20, 300)      241500      ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " bilstm (Bidirectional)         (None, 20, 128)      186880      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 128)          148         ['bilstm[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['attention[0][0]']              \n",
      "                                                                                                  \n",
      " intent (Dense)                 (None, 5)            645         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " intent_features (Dense)        (None, 32)           192         ['intent[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 160)          0           ['dropout[0][0]',                \n",
      "                                                                  'intent_features[0][0]']        \n",
      "                                                                                                  \n",
      " subcategory (Dense)            (None, 19)           3059        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " sentiment (Dense)              (None, 3)            387         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 432,811\n",
      "Trainable params: 191,311\n",
      "Non-trainable params: 241,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight',\n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias',\n",
    "                                 shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)  # (batch_size, time_steps, 1)\n",
    "        a = K.softmax(e, axis=1)               # (batch_size, time_steps, 1)\n",
    "        weighted_input = x * a                 # Element-wise multiplication with attention weights\n",
    "        return K.sum(weighted_input, axis=1)   # Sum over time steps\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "# Model parameters.\n",
    "lstm_units = 64\n",
    "dropout_rate = 0.5\n",
    "intent_feature_dim = 32  # Dimension for intent-based features.\n",
    "\n",
    "# Input layer for padded sequences.\n",
    "input_seq = Input(shape=(max_len,), name='input')\n",
    "\n",
    "# Embedding layer using the pre-trained (augmented) GoogleNews embedding matrix.\n",
    "embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                            output_dim=embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len,\n",
    "                            trainable=False,\n",
    "                            name=\"embedding\")(input_seq)\n",
    "\n",
    "# Bidirectional LSTM that returns sequences (for the attention mechanism).\n",
    "bi_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True), name=\"bilstm\")(embedding_layer)\n",
    "\n",
    "# Apply the custom attention layer.\n",
    "attn = Attention(name=\"attention\")(bi_lstm)\n",
    "\n",
    "# Add dropout for regularization.\n",
    "x = Dropout(dropout_rate, name=\"dropout\")(attn)\n",
    "\n",
    "# First branch: High-level Intent prediction.\n",
    "intent_output = Dense(len(intent_classes), activation='softmax', name='intent')(x)\n",
    "\n",
    "# Transform intent predictions into a feature vector.\n",
    "intent_features = Dense(intent_feature_dim, activation='relu', name=\"intent_features\")(intent_output)\n",
    "\n",
    "# Concatenate the base representation with the intent-based features for subcategory prediction.\n",
    "combined = Concatenate(name=\"concatenate\")([x, intent_features])\n",
    "\n",
    "# Second branch: Subcategory prediction.\n",
    "subcategory_output = Dense(len(subcategory_classes), activation='softmax', name='subcategory')(combined)\n",
    "\n",
    "# Third branch: Sentiment prediction.\n",
    "# Here, we use the same base representation (x) from the attention output.\n",
    "sentiment_output = Dense(len(sentiment_classes), activation='softmax', name='sentiment')(x)\n",
    "\n",
    "# Create and compile the model with three outputs.\n",
    "model = Model(inputs=input_seq, outputs=[intent_output, subcategory_output, sentiment_output])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c31e7598-d757-45f2-853f-fc1456a53e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 2s 21ms/step - loss: 4.6737 - intent_loss: 0.9868 - subcategory_loss: 2.4221 - sentiment_loss: 1.2648 - intent_accuracy: 0.6442 - subcategory_accuracy: 0.2211 - sentiment_accuracy: 0.5074 - val_loss: 3.0539 - val_intent_loss: 0.4969 - val_subcategory_loss: 1.8629 - val_sentiment_loss: 0.6942 - val_intent_accuracy: 0.8421 - val_subcategory_accuracy: 0.5263 - val_sentiment_accuracy: 0.7368\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 3.1996 - intent_loss: 0.5057 - subcategory_loss: 1.9036 - sentiment_loss: 0.7903 - intent_accuracy: 0.8021 - subcategory_accuracy: 0.3200 - sentiment_accuracy: 0.6505 - val_loss: 2.3580 - val_intent_loss: 0.3764 - val_subcategory_loss: 1.6053 - val_sentiment_loss: 0.3763 - val_intent_accuracy: 0.7895 - val_subcategory_accuracy: 0.5263 - val_sentiment_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 2.6209 - intent_loss: 0.3351 - subcategory_loss: 1.6303 - sentiment_loss: 0.6555 - intent_accuracy: 0.8905 - subcategory_accuracy: 0.4442 - sentiment_accuracy: 0.7411 - val_loss: 1.9143 - val_intent_loss: 0.2706 - val_subcategory_loss: 1.3548 - val_sentiment_loss: 0.2889 - val_intent_accuracy: 0.8421 - val_subcategory_accuracy: 0.5789 - val_sentiment_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 2.2744 - intent_loss: 0.2560 - subcategory_loss: 1.4156 - sentiment_loss: 0.6027 - intent_accuracy: 0.9179 - subcategory_accuracy: 0.5305 - sentiment_accuracy: 0.7537 - val_loss: 1.7062 - val_intent_loss: 0.2114 - val_subcategory_loss: 1.1497 - val_sentiment_loss: 0.3451 - val_intent_accuracy: 0.9474 - val_subcategory_accuracy: 0.8947 - val_sentiment_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.9322 - intent_loss: 0.2113 - subcategory_loss: 1.1905 - sentiment_loss: 0.5305 - intent_accuracy: 0.9242 - subcategory_accuracy: 0.6400 - sentiment_accuracy: 0.8147 - val_loss: 1.3869 - val_intent_loss: 0.1250 - val_subcategory_loss: 0.9648 - val_sentiment_loss: 0.2972 - val_intent_accuracy: 0.9474 - val_subcategory_accuracy: 0.8947 - val_sentiment_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6552 - intent_loss: 0.1417 - subcategory_loss: 1.0495 - sentiment_loss: 0.4639 - intent_accuracy: 0.9726 - subcategory_accuracy: 0.6589 - sentiment_accuracy: 0.8316 - val_loss: 1.1329 - val_intent_loss: 0.1020 - val_subcategory_loss: 0.8409 - val_sentiment_loss: 0.1900 - val_intent_accuracy: 0.9474 - val_subcategory_accuracy: 0.9474 - val_sentiment_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.4685 - intent_loss: 0.1173 - subcategory_loss: 0.9360 - sentiment_loss: 0.4151 - intent_accuracy: 0.9663 - subcategory_accuracy: 0.7221 - sentiment_accuracy: 0.8421 - val_loss: 1.0103 - val_intent_loss: 0.0800 - val_subcategory_loss: 0.7362 - val_sentiment_loss: 0.1941 - val_intent_accuracy: 0.9474 - val_subcategory_accuracy: 0.8947 - val_sentiment_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.2749 - intent_loss: 0.1161 - subcategory_loss: 0.7959 - sentiment_loss: 0.3629 - intent_accuracy: 0.9684 - subcategory_accuracy: 0.7958 - sentiment_accuracy: 0.8653 - val_loss: 0.9638 - val_intent_loss: 0.0933 - val_subcategory_loss: 0.6833 - val_sentiment_loss: 0.1872 - val_intent_accuracy: 0.9474 - val_subcategory_accuracy: 0.8947 - val_sentiment_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.1999 - intent_loss: 0.1229 - subcategory_loss: 0.7454 - sentiment_loss: 0.3316 - intent_accuracy: 0.9621 - subcategory_accuracy: 0.8168 - sentiment_accuracy: 0.9095 - val_loss: 0.8032 - val_intent_loss: 0.0749 - val_subcategory_loss: 0.5826 - val_sentiment_loss: 0.1457 - val_intent_accuracy: 0.9474 - val_subcategory_accuracy: 1.0000 - val_sentiment_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.9998 - intent_loss: 0.0835 - subcategory_loss: 0.6328 - sentiment_loss: 0.2835 - intent_accuracy: 0.9768 - subcategory_accuracy: 0.8295 - sentiment_accuracy: 0.9074 - val_loss: 0.6456 - val_intent_loss: 0.0290 - val_subcategory_loss: 0.4769 - val_sentiment_loss: 0.1397 - val_intent_accuracy: 1.0000 - val_subcategory_accuracy: 1.0000 - val_sentiment_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    [y_intent_train, y_sub_train, y_sentiment_train],\n",
    "    validation_data=(X_test, [y_intent_test, y_sub_test, y_sentiment_test]),\n",
    "    epochs=10,\n",
    "    batch_size=16\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
